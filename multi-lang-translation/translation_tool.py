# Gerekli k√ºt√ºphaneleri i√ße aktar
import json
import requests
import time
import os

# Hugging Face eri≈üim tokenƒ±nƒ± ortam deƒüi≈ükeninden al
HF_TOKEN = os.getenv("HF_TOKEN")

# API √ßaƒürƒ±larƒ±nda kullanƒ±lacak yetkilendirme ba≈ülƒ±klarƒ±
headers = {
    "Authorization": f"Bearer {HF_TOKEN}"
}

# Ana modellerin API URL'leri (TR‚ÜíEN ve EN‚ÜíTR)
MODELS = {
    "tr_to_en": "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-tr-en",
    "en_to_tr": "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-tc-big-en-tr"
}

# Yedek modellerin API URL'leri (√∂zellikle EN‚ÜíTR i√ßin)
BACKUP_MODELS = {
    "en_to_tr_1": "https://api-inference.huggingface.co/models/facebook/mbart-large-50-many-to-many-mmt",
    "en_to_tr_2": "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-mul-tr",
    "en_to_tr_3": "https://api-inference.huggingface.co/models/google/mt5-base"
}

# Soru e≈üle≈ütirmeleri i√ßin normalizasyon fonksiyonu
def normalize(text):
    return text.strip().lower().replace("'", "'").replace("‚Äú", "\"").replace("‚Äù", "\"")

# LLM ile √ßeviri yapan ana fonksiyon
def translate_with_llm(text, source_lang, target_lang, max_retries=3):
    if source_lang == "tr" and target_lang == "en":
        model_url = MODELS["tr_to_en"]
        direction = "TR‚ÜíEN"
        payload = {"inputs": text}
    elif source_lang == "en" and target_lang == "tr":
        model_url = MODELS["en_to_tr"]
        direction = "EN‚ÜíTR"
        payload = {"inputs": text}
    else:
        return None
    
    # Maksimum deneme sayƒ±sƒ± kadar modeli √ßalƒ±≈ütƒ±rmayƒ± dene
    for attempt in range(max_retries):
        try:
            print(f"üîÑ {direction} √ßevirisi: {text[:40]}..." + (f" (deneme {attempt+1})" if attempt > 0 else ""))
            response = requests.post(model_url, headers=headers, json=payload, timeout=35)
            
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0 and "translation_text" in result[0]:
                    translation = result[0]["translation_text"].strip()
                    if translation and len(translation) > 1:
                        return translation
            elif response.status_code == 503:
                time.sleep(30)
                continue
            elif response.status_code == 429:
                time.sleep(25)
                continue
            elif response.status_code == 404:
                if source_lang == "en" and target_lang == "tr":
                    for backup_url in BACKUP_MODELS.values():
                        backup_payload = {
                            "inputs": text,
                            "parameters": {"src_lang": "en_XX", "tgt_lang": "tr_TR"}
                        } if "mbart" in backup_url else payload

                        backup_response = requests.post(backup_url, headers=headers, json=backup_payload, timeout=40)
                        if backup_response.status_code == 200:
                            backup_result = backup_response.json()
                            if isinstance(backup_result, list) and len(backup_result) > 0 and "translation_text" in backup_result[0]:
                                return backup_result[0]["translation_text"].strip()
                return None
        except requests.exceptions.Timeout:
            print(f"‚è∞ Timeout (deneme {attempt+1})")
        except Exception as e:
            print(f"‚ö†Ô∏è Hata: {str(e)[:100]}")
        
        if attempt < max_retries - 1:
            time.sleep((attempt + 1) * 8)
    
    return None

# Ana √ßeviri i≈ülemlerinin yapƒ±ldƒ±ƒüƒ± fonksiyon
def process_translations():
    try:
        from google.colab import files
        uploaded = files.upload()
        file_name = list(uploaded.keys())[0]
    except:
        file_name = input("üìÇ L√ºtfen i≈ülenecek JSON dosyasƒ±nƒ±n adƒ±nƒ± girin: ").strip()

    with open(file_name, encoding="utf-8") as f:
        data = json.load(f)

    translated_count = 0
    failed_count = 0

    # Her sayfa veya i√ßerik i√ßin d√∂n
    for item in data:
        # ƒ∞ngilizce ve T√ºrk√ße i√ßerikleri al
        en_entries = item.get("en", [])
        tr_entries = item.get("tr", [])
        en_map = {normalize(e["question"]): e for e in en_entries}
        tr_map = {normalize(e["question"]): e for e in tr_entries}

        # T√ºrk√ße sorular arasƒ±nda ƒ∞ngilizcesi eksik olanlarƒ± bul
        for norm_q, tr_entry in tr_map.items():
            if norm_q not in en_map:
                translated_q = translate_with_llm(tr_entry["question"], "tr", "en")
                time.sleep(3)
                if not translated_q:
                    failed_count += 1
                    continue
                translated_a = translate_with_llm(tr_entry["answer"], "tr", "en")
                time.sleep(3)
                if translated_a:
                    en_entries.append({
                        "page_url": tr_entry.get("page_url", item.get("url", "")),
                        "page_title": tr_entry.get("page_title", ""),
                        "language": "en", 
                        "modified_date": "Translated by Helsinki-NLP",
                        "question": translated_q,
                        "answer": translated_a
                    })
                    translated_count += 1
                else:
                    failed_count += 1

        # ƒ∞ngilizce sorular arasƒ±nda T√ºrk√ßesi eksik olanlarƒ± bul
        for norm_q, en_entry in en_map.items():
            if norm_q not in tr_map:
                translated_q = translate_with_llm(en_entry["question"], "en", "tr")
                time.sleep(3)
                if not translated_q:
                    failed_count += 1
                    continue
                translated_a = translate_with_llm(en_entry["answer"], "en", "tr")
                time.sleep(3)
                if translated_a:
                    tr_entries.append({
                        "page_url": en_entry.get("page_url", item.get("url", "")),
                        "page_title": en_entry.get("page_title", ""),
                        "language": "tr",
                        "modified_date": "Helsinki-NLP tarafƒ±ndan √ßevrildi", 
                        "question": translated_q,
                        "answer": translated_a
                    })
                    translated_count += 1
                else:
                    failed_count += 1

        # G√ºncellenmi≈ü i√ßerikleri tekrar yerle≈ütir
        item["en"] = en_entries
        item["tr"] = tr_entries

    # √áevirisi tamamlanan verileri kaydet
    output_filename = "completed_translations.json"
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ √áeviri tamamlandƒ±. Ba≈üarƒ±lƒ±: {translated_count}, Ba≈üarƒ±sƒ±z: {failed_count}")
    print(f"üíæ Kayƒ±t dosyasƒ±: {output_filename}")

# Script doƒürudan √ßalƒ±≈ütƒ±rƒ±ldƒ±ƒüƒ±nda buradan ba≈ülar
if __name__ == "__main__":
    if not HF_TOKEN:
        print("‚ùå HATA: HF_TOKEN ortam deƒüi≈ükeni eksik!")
        print("üîó Token almak i√ßin: https://huggingface.co/settings/tokens")
    else:
        process_translations()

